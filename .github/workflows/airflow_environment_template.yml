name: Deploy Airflow Environment Components
# This workflow defines the actual deployment steps (checkout, authentication, DAG synchronization, data pipeline sync) as a reusable template. 
# Accepts environment-specific parameters (like bucket name, environment) from the calling workflow and performs the deployment to Cloud Composer.

on:
  workflow_call:
    inputs:
      environment:
        required: true
        type: string
    secrets:
      WIF_PROVIDER:
        required: true
      WIF_SERVICE_ACCOUNT:
        required: true

jobs:
  environment-deployment:
    name: Deploy ${{ inputs.environment }} Environment to Cloud Composer
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    permissions:
      contents: 'read'
      id-token: 'write'

    env:
      COMPOSER_BUCKET: ${{ vars.COMPOSER_BUCKET }} 
    
    steps: 
      - name: Check deployment branch
        run: echo "Deploying to ${{ inputs.environment }} Cloud Composer Environment from branch ${{ github.ref }}"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud via Workload Identity
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
          service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }}

      - name: Set up gcloud
        uses: google-github-actions/setup-gcloud@v2

      - name: Sync Airflow DAGs to Cloud Composer GCS Bucket
        run: |
          LOCAL_DAGS=$(ls dags/ | grep '\.py$' | awk -F'/' '{print $NF}' | sort )
          echo -e "DAGs in Github repository \n$LOCAL_DAGS"
          GCS_DAGS=$(gcloud storage ls "${{ env.COMPOSER_BUCKET }}/dags/" | grep '\.py$' | awk -F'/' '{print $NF}' | sort || true)
          echo -e "Airflow DAGs in Cloud Storage of Cloud Composer ${{ inputs.environment }} environment \n$GCS_DAGS"
          gcloud storage rsync --recursive --checksums-only dags/ ${{ env.COMPOSER_BUCKET }}/dags --verbosity info

      - name: Sync DBT Data Pipeline to Cloud Composer GCS Bucket
        run: |
          if [ -d "im_data_pipeline" ]; then
            echo "Syncing im_data_pipeline folder to Cloud Composer GCS bucket..."

            gcloud storage rsync --recursive --checksums-only \
              --exclude=".gitkeep" \
              --exclude=".gitignore" \
              --exclude=".git*" \
              --exclude="*.tmp" \
              --exclude="*.log" \
              --exclude=".DS_Store" \
              im_data_pipeline/ "${{ env.COMPOSER_BUCKET }}/data/im_data_pipeline" --verbosity info

            echo "Successfully synced im_data_pipeline to ${{ env.COMPOSER_BUCKET }}/data (excluding git/temp files)"
          else
            echo "im_data_pipeline folder not found, skipping data pipeline sync"
          fi
